# -*- coding: utf-8 -*-
""" Result.Result class to wrap the simulation results."""
from __future__ import division, print_function  # Python 2 compatibility

__author__ = "Lilian Besson"
__version__ = "0.9"

import numpy as np


class Result(object):
    """ Result accumulators."""

    # , delta_t_save=1):
    def __init__(self, nbArms, horizon, indexes_bestarm=-1, means=None, isHMAB = False, **kwargs):
        """ Create ResultMultiPlayers."""
        self.isHMAB = isHMAB
        if isHMAB:
            if "nbBands" in kwargs and "nbBins" in kwargs:
                self.nbBands = kwargs["nbBands"]
                self.nbBins = kwargs["nbBins"]
            else:
                print("!!NEED TO SPECIFY THE NUMBER OF BANDS AND BINS IN RESULT OBJECT")
            self.choicesBB = np.zeros((2,horizon))  # Storing Band/Bin choice for each time step
            self.pullsBB = np.zeros((self.nbBands,self.nbBins), dtype=int)  # Storing the number of pulls for each Bin/Ban


        self.choices = np.zeros(horizon, dtype=int)  #: Store all the choices.
        self.pulls = np.zeros(nbArms, dtype=int)  #: Store the pulls.

        # self._means = means  # Keep the means for ChangingAtEachRepMAB cases
        # self.delta_t_save = delta_t_save  #: Sample rate for saving.
        self.rewards = np.zeros(horizon)  #: Store all the rewards, to compute the mean.
        if means is not None:
            indexes_bestarm = np.nonzero(np.isclose(means, np.max(means)))[0]
        indexes_bestarm = np.asarray(indexes_bestarm)
        if np.size(indexes_bestarm) == 1:
            indexes_bestarm = np.asarray([indexes_bestarm])
        self.indexes_bestarm = [ indexes_bestarm for _ in range(horizon)]  #: Store also the position of the best arm, XXX in case of dynamically switching environment.
        self.running_time = -1  #: Store the running time of the experiment.
        self.memory_consumption = -1  #: Store the memory consumption of the experiment.
        self.number_of_cp_detections = 0  #: Store the number of change point detected during the experiment.

    def store(self, time, choice, reward):
        """ Store results."""
        if isinstance(reward, (np.ndarray,tuple,list)):
            num_rewards = reward.__len__()
            self.choices[time:time+num_rewards] = choice
            self.rewards[time:time+num_rewards] = reward
            self.pulls[choice] += num_rewards
            (Band, Bin) = np.unravel_index(choice, (self.nbBands,self.nbBins))
            for t in range(time, time+num_rewards):
                self.choicesBB[:,t] = (Band, Bin)
            # self.choicesBB[:,time:time+num_rewards] = (Band,Bin)*5

        else:
            self.choices[time] = choice
            self.rewards[time] = reward
            self.pulls[choice] += 1
            (Band, Bin) = np.unravel_index(choice, (self.nbBands, self.nbBins))
            self.choicesBB[:,time] = (Band, Bin)



    def change_in_arms(self, time, indexes_bestarm):
        """ Store the position of the best arm from this list of arm.

        - From that time t **and after**, the index of the best arm is stored as ``indexes_bestarm``.

        .. warning:: FIXME This is still experimental!
        """
        for t in range(time, len(self.indexes_bestarm)):
            self.indexes_bestarm[t] = indexes_bestarm

    def updateT(self,t):
        self.choices = self.choices[:t]
        self.rewards = self.rewards[:t]
        return 0
