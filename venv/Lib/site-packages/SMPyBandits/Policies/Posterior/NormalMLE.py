# -*- coding: utf-8 -*-
""" Manipulate a posterior of Gaussian experiments, which happens to also be a Gaussian distribution if the prior is Gaussian. *Easy peasy!*

.. warning:: TODO I have to test it!

- Reference: [[*Further optimal regret bounds for Thompson sampling*, S. Agrawal and N. Goyal, In Artificial Intelligence and Statistics, pages 99â€“107, 2013.](http://proceedings.mlr.press/v31/agrawal13a.pdf)]
"""
from __future__ import division, print_function  # Python 2 compatibility

__author__ = "Lilian Besson"
__version__ = "0.9"

import numpy as np
from scipy.stats import norm


try:
    from numpy.random import normal as normalvariate  # Faster! Yes!
except ImportError:
    from random import gauss as normalvariate

from scipy.special import nrdtrimn, nrdtrisd


# Local imports
from .Posterior import Posterior

class NormalMLE(Posterior):

    def __init__(self, paramDict, i=0):
        if isinstance(paramDict["nbBins"], list):
            self.nbBins = paramDict["nbBins"][i]
        else:
            self.nbBins = paramDict["nbBins"]
        self.binSums = np.zeros(self.nbBins)
        self.binCounts = np.zeros(self.nbBins)
        self.binMeans = np.zeros(self.nbBins)
        self.mu = 0
        self.sigma = 1

    def update(self, obs, bin):
        self.binCounts[bin] += 1
        self.binSums[bin] += obs
        self.binMeans[bin] = self.binSums[bin]/self.binCounts[bin]
        self.mu = np.mean(self.binMeans)
        self.sigma = np.std(self.binMeans)

    def getRV(self):
        self.RV = norm(loc = self.mu, scale = self.sigma)
        return self.RV




class NormalMLE2(Posterior):


    def __init__(self, mu=0.0, sigma = 1):
        r"""Create a posterior assuming the prior is :math:`\mathcal{N}(\mu, 1)`.
        
        - The prior is centered (:math:`\mu=1`) by default, but parameter ``mu`` can be used to change this default.
        """
        self._mu = float(mu)  # initial value
        self.mu = float(mu)   #: Parameter :math:`\mu` of the posterior
        self.sigma = sigma  #: The parameter :math:`\sigma` of the posterior
        self._sigma = sigma
        # internal memories
        self._nb_data = 0  # number of samples!
        self._sum_data = 0.0  # sum of samples!
        self._sumsq_data = 0.0

    def __str__(self):
        return "Gauss({:.3g}, {:.3g})".format(self.mu, self.sigma)

    def reset(self, mu=None):
        r""" Reset the for parameters :math:`\mu, \sigma`, as when creating a new Gauss posterior."""
        if mu is None:
            self.mu = self._mu
        self.sigma = self._nu

    def sample(self):
        r""" Get a random sample :math:`(x, \sigma^2)` from the Gaussian posterior (using :func:`scipy.stats.invgamma` for the variance :math:`\sigma^2` parameter and :func:`numpy.random.normal` for the mean :math:`x`).

        - Used only by :class:`Thompson` Sampling and :class:`AdBandits` so far.
        """
        return normalvariate(loc=self.mu, scale=self.sigma)

    def quantile(self, p):
        """ Return the p-quantile of the Gauss posterior.

        .. note:: It now works fine with :class:`Policies.BayesUCB` with Gauss posteriors, even if it is MUCH SLOWER than the Bernoulli posterior (:class:`Gamma`).
        """
        quantile_on_x = nrdtrimn(p, 1, self.sigma)
        quantile_on_sigma2 = nrdtrisd(p, 1, self.mu)
        return quantile_on_x * quantile_on_sigma2

    def mean(self):
        r""" Compute the mean, :math:`\mu` of the Gauss posterior (should be useless)."""
        return self.mu

    def variance(self):
        r""" Compute the variance, :math:`\sigma`, of the Gauss posterior (should be useless)."""
        return self.sigma

    def update(self, obs):
        r"""Add an observation :math:`x` or a vector of observations, assumed to be drawn from an unknown normal distribution.
        """
        # print("Info: calling Gauss.update() with obs = {} ...".format(obs))  # DEBUG
        # one more observation!
        self._nb_data += 1
        self._sum_data += float(obs)
        self._sumsq_data += obs*obs
        self.mu = self._sum_data/self._nb_data
        self.sigma = np.sqrt(self._sumsq_data-(self._sum_data*self._sum_data/self._nb_data)/(self._nb_data-1))

    def forget(self, obs):
        """Forget the last observation. Should work, but should also not be used..."""
        raise NotImplementedError
