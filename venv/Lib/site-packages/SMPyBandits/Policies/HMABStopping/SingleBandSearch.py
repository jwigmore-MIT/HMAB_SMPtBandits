from SMPyBandits.Policies.HMABPolicy import HMABPolicy, confidence_interval
from scipy.stats import norm

import numpy as np

class SingleBandSearch(HMABPolicy):

    def __init__(self,nbBands, nbBins, DistributionSetup = None, verbose = True, AlgoParams = None):
        super().__init__(nbBands, nbBins, DistributionSetup, verbose)
        self.AlgoParams = AlgoParams
        self.percentile = AlgoParams["percentile"]
        self.confidence = AlgoParams["confidence"]
        self.init_distributions()
        self.nbDraws = 2
        self.qStar = self.Bands[0].q

    def init_distributions(self):
        self.BandDist = self.DistributionSetup["BandDist"]
        for B in range(self.nbBands):
            # Using frequentist estimation so priors/posteriors not needed
            # self.posteriors[B] = self.BayesSetup["hyperprior"](self.BayesSetup["hyperparameters"])
            CI = self.AlgoParams["confidence"].get("Bands", False)
            self.Bands[B] = self.Band(self, B, self.nbBins, self.DistributionSetup["BandDist"],
                                      self.DistributionSetup["InitBandParams"], CI = CI)
            for b in range(self.nbBins):
                CI = self.AlgoParams["confidence"].get("bins", False)
                self.Bands[B].Bins[b] = self.Bin(self, self.Bands[B], b, self.DistributionSetup["BinDist"],
                                                 self.DistributionSetup["InitBinParams"], CI = CI)
                self.Bins[self.getArm(B, b)] = self.Bands[B].Bins[b]
                # self.EstBinDist[self.getArm(B,b)] = self.BayesSetup["BinDist"](self.BayesSetup["InitBinParams"])
    def checkStop(self):
        if self.Bands[0].BinSampled.__len__() > 0:
            GreatestLowerCI_bin = np.argmax([self.Bands[0].BinLowerCI[x] for x in self.Bands[0].BinSampled])
            GreatestLowerCI_value = self.Bands[0].BinLowerCI[self.Bands[0].BinSampled[GreatestLowerCI_bin]]
            if GreatestLowerCI_value >= self.qStar:
                self.STOP = True
                self.ChosenBin = self.getArm(0,self.Bands[0].BinSampled[GreatestLowerCI_bin])
        return self.STOP

    def choice(self):
        if self.Bands[0].BinSampled.__len__() > 0:
            valid_bins = self.Bands[0].BinSampled
            valid_UCI = np.array([self.Bands[0].BinUpperCI[x] for x in valid_bins])
            valid_LCI = np.array([self.Bands[0].BinLowerCI[x] for x in valid_bins])
            GreatestUpperCI_bin = np.argmax([self.Bands[0].BinUpperCI[x] for x in valid_bins])
            GreatestUpperCI_value = self.Bands[0].BinUpperCI[self.Bands[0].BinSampled[GreatestUpperCI_bin]]

        else:
            GreatestUpperCI_value = -np.infty

        if GreatestUpperCI_value >= self.qStar:
            choice = self.getArm(0,self.Bands[0].BinSampled[GreatestUpperCI_bin])
        else:
            bin = np.random.choice(self.nbBins)
            while bin in self.Bands[0].BinSampled:
                bin = np.random.choice(self.nbBins)
            choice = self.getArm(0, bin)
        return choice






    # Basically Overriding all Band/Bin aspects of HMABPolicy
    class Band:
        def __init__(self, Policy, index, nbBins, distribution, initParams, CI):
            self.Policy = Policy
            self.nbBins = nbBins
            self.index = index
            self.Dist = distribution(initParams)
            self.Mu = initParams[0]
            self.Sigma = initParams[1]
            self.Bins = [None] * nbBins
            self.pulls = 0
            self.sumRewards = 0
            self.BinSampled = []
            self.estBinMeans = [None] * nbBins
            self.BinLowerCI = [None]*nbBins
            self.BinUpperCI = [None]*nbBins
            self.percentile = None
            self.CI = CI
            self.q = norm.ppf(self.Policy.percentile, self.Mu, self.Sigma)

        def updateDistribution(self): #this is changed to still estimate just to judge the estimation trends
            # binMeans = np.array([self.estBinMeans[x] for x in np.where(self.BinSampled)[0].tolist()])
            binMeans = [self.estBinMeans[x] for x in self.BinSampled]
            self.Mu_est = np.mean(binMeans)
            self.Sigma_est = np.std(binMeans)
            #self.Dist.reset(self.Mu, self.Sigma, 0, 0, 0, 0)

        def getReward(self, bin, reward):
            if isinstance(reward, float): reward = np.array([reward])
            self.pulls += reward.shape[0]
            self.sumRewards += np.sum(reward)
            self.Bins[bin].addReward(reward)
            self.updateDistribution() #Will still estimate, but it won't effect anything
            self.estimatePercentile()

        def estimatePercentile(self):
            self.percentile = self.Dist.getPercentile(
                self.Policy.percentile)  ## Based on the current estimated distribution, get the Policy.quantile value
            return self.percentile

        def computeLawless(self, confidence):
            # estimates CI on the pth percentile
            Zp = stat.norm.ppf(confidence)
            # self.mu - stats.t

        def __str__(self):
            return f"Band [{self.index}] ED ~  {self.Dist.__str__()}"

        def plotSampledBins(self, ax):
            for b in self.BinSampled:
                ax = self.Bins[b].plot(ax)
            ax.axvline(x=self.q,linestyle = "--")
            return ax


    class Bin:
        def __init__(self, Policy, Band, bindex, Distribution, initParams, CI):
            self.Policy = Policy
            self.Band = Band
            self.bindex = bindex
            self.Distribution = Distribution(initParams)
            self.pulls = 0
            self.sumReward = 0
            self.sumRewardSq = 0
            self.rewardHistory = []
            self.mu = initParams[0]
            self.sigma = initParams[1]
            self.sampled = False
            self.CI = CI

        def addReward(self, reward):
            if isinstance(reward, float): reward = np.array([reward])
            self.pulls += reward.shape[0]
            self.sumReward += np.sum(reward)
            self.sumRewardSq += np.sum(np.square(reward))  # rewards squared, needed for variance estimation
            self.rewardHistory.extend(reward.tolist())

            if self.CI is not False:  # If there is a value given for CI, compute the confidence interval
                mean, std = confidence_interval(self.rewardHistory, self.CI)
                self.mu, self.mu_l, self.mu_u = mean
                self.sigma, self.sigma_l, self.sigma_u = std
                self.Distribution.reset(self.mu, self.sigma, self.mu_l, self.mu_u, self.sigma_l, self.sigma_u)
                self.Band.BinLowerCI[self.bindex] = self.mu_l
                self.Band.BinUpperCI[self.bindex] = self.mu_u
            else:
                self.mu = self.sumReward / self.pulls
                self.sigma = np.sqrt((self.sumRewardSq - (self.sumReward ** 2) / self.pulls) / (self.pulls - 1))
                self.Distribution.reset(self.mu, self.sigma)
            if self.sampled is False:
                self.sampled = True
                self.Band.BinSampled.append(self.bindex)
            self.Band.estBinMeans[self.bindex] = self.mu

        def __str__(self):
            return f"({self.Band.index, self.bindex}) ~ {self.Distribution.__str__()}"

        def plot(self, ax, label=None):
            if self.CI is not False:
                self.Distribution.plot(ax, UCI = self.mu_u, LCI = self.mu_l)
            else:
                self.Distribution.plot(ax)
            return ax

