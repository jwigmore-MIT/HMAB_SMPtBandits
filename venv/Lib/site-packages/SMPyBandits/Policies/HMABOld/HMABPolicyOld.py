"""
Based class for al Hierarchical MAB (HMAB) policies
"""
from SMPyBandits.Policies.Posterior import NormalGamma
from SMPyBandits.Policies.Posterior.Uniform import Uniform
from SMPyBandits.Policies.BasePolicy import BasePolicy
import numpy as np

class HMABPolicyOld(BasePolicy):
    def __init__(self, nbBands, nbBins, posterior = NormalGamma, priorParams = None, verbose = True, estMethod = 1):
        """

        :param nbBands:
        :param nbBins:
        :param posterior: <SMPyBandits Posterior>
        :param priorParams: <dict> of initial parameters for our posterior
        """
        self.nbBands = nbBands
        self.nbBins = nbBins
        self.nbArms = nbBins*nbBands  # Arms is the linear version of Band/Bin indexing
        self.posteriorDist = posterior
        self.posteriorBand = [None]*nbBands
        self.likelihoodParams = [None]*nbBands
        self.isHMAB = True
        self.verbose = verbose
        self.estMethod = estMethod

        ### NOT TO BE EDITED - used by evaluator functions
        # ________________________________________________
        self.t = -1  #: Internal time
        self.pulls = np.zeros(self.nbArms, dtype= int)      #Count of pulls for each arm
        self.rewards = np.zeros(self.nbArms)    # Sum of rewards accumulated for each arm
        # ________________________________________________
        ##

        # Parameter Estimator
        self.lam_est = np.zeros(self.nbArms)    # Estimate of each arm mean

        # For recording actions/rewards in Band/Bin format
        self.pullsBB = np.zeros((nbBands, nbBins), dtype=int)  #: Number of pulls of each band and bin
        self.rewardsBB = np.zeros((nbBands, nbBins))   #: Cumulated rewards of each arms
        self.lam_estBB = np.zeros((nbBands, nbBins))   #: Estimated expected reward for the bin based on observations


        #self.Mu_est = np.zeros(nbBands)
        print(f"Initializing the posteriors for all {self.nbBands} Bands")
        if not isinstance(list(priorParams.values())[0],list):       # checks if first value in dictionary is an array of parameters, if not initializes all posteriors the same
            print(f"Assigning all posteriors the parameters:")
            print(priorParams)
            for Band in range(nbBands):
                self.posteriorBand[Band] = posterior(priorParams)
            print(f"Assigning all posterior parameters individually")
            for Band in range(nbBands):
                self.posteriorBand[Band] = posterior(priorParams, Band)


        for Band in range(nbBands):
            self.likelihoodParams[Band] = self.posteriorBand[Band].likelihoodParams
            #print(f"""The POSTERIOR for Band ({Band}) is ~ {self.posteriorBand[Band].__str__()}""")
            #print(f"""The LIKELIHOOD for Band ({Band}) is ~ Normal{self.likelihoodParams[Band]}""")
        self.printParams()

    def getArm(self,Band, Bin):
        return np.ravel_multi_index((Band, Bin), (self.nbBands,self.nbBins))

    def getBB(self,arm):
        return np.unravel_index(arm, (self.nbBands,self.nbBins))

    def printParams(self):
        for Band in range(self.nbBands):
            print("*"*20)
            print(f"""The POSTERIOR for Band ({Band}) is ~ {self.posteriorBand[Band].__str__()}""")
            print(f"""The LIKELIHOOD for Band ({Band}) is:""")
            print(self.posteriorBand[Band].likelihoodParams)

    def __str__(self):
        """ -> str"""
        return self.__class__.__name__

    def startGame(self):
        """ Start the game (fill pulls and rewards with 0)."""
        self.t = 0
        self.pullsBB.fill(0)
        self.rewardsBB.fill(0)
        self.pulls.fill(0)
        self.rewards.fill(0)

    def getReward(self, choice, reward):
        """ Give a reward: increase t, pulls, and update cumulated sum of rewards for that arm (normalized in [0, 1])."""
        (Band, Bin) = self.getBB(choice)
        self.t += 1
        self.updatePulls(Band, Bin)
        self.updateReward(Band, Bin, reward, lam_est=True)  # update reward and estimated expected reward for bin
        # band_mu_est = np.mean(self.lam_est[Band,:])
        # band_sig_est = np.std(self.lam_est[Band,:])
        if self.estMethod == 1:
            self.posteriorBand[Band].update(
                self.lam_estBB[Band, Bin])  # This updates the posterior using a single bin estimate
        elif self.estMethod == 2:
            self.posteriorBand[Band].update2(self.lam_estBB[Band, :])
        elif self.estMethod == 3:
            self.posteriorBand[Band].update3(self.lam_estBB[Band, :])
        else:
            print("error: need estimation method specified for policy")



        self.likelihoodParams[Band] = self.posteriorBand[Band].likelihoodParams
        if self.verbose:
            print("*" * 20)
            print(f"Chosen (Band/Bin):({Band},{Bin})")
            print(f"Received Reward: {reward}")
            print(f"Estimated lambda's: {self.lam_estBB[Band, :]}")
            print(f"New Band Likelihood Params:")
            print(self.likelihoodParams[Band])
            print("*" * 20)
            print()

    def choice(self): #Need this to remain choice for evaluator()
        """ Not defined."""
        raise NotImplementedError(
            "This method choice() has to be implemented in the child class inheriting from BasePolicy.")



    def computeArmIndex(self, Band, Bin):  # Compute the arm index (i.e. Upper Conf. Bounds) for ArmBelief(Band,Bin)
        if self.pullsBB[Band, Bin] < 1:
            return float('+inf')
        else:
            #return (self.rewards[Band, Bin] / self.pulls[Band, Bin]) + np.sqrt((2 * log(self.t)) / self.pulls[Band, Bin]) # Considering all pulls
            return (self.rewardsBB[Band, Bin] / self.pullsBB[Band, Bin]) + np.sqrt((2 * np.log(np.sum(self.pullsBB[Band,:]))) / self.pullsBB[Band, Bin])  # Considering only pulls within bin

    def computeBandArmIndex(self,Band):
        indexes = np.zeros(self.nbBins)
        for Bin in range(self.nbBins):
            indexes[Bin] = self.computeArmIndex(Band, Bin)
        self.indexes = indexes

    def updatePulls(self, Band, Bin):
        self.pullsBB[Band, Bin] += 1
        self.pulls[self.getArm(Band, Bin)] +=1

    def updateReward(self, Band, Bin, reward, lam_est = True):
        self.rewardsBB[Band, Bin] += reward
        self.rewards[self.getArm(Band, Bin)] += reward
        if lam_est:
            self.lam_estBB[Band, Bin] = self.rewardsBB[Band, Bin] / self.pullsBB[
                Band, Bin]
            self.lam_est[self.getArm(Band, Bin)] = self.lam_estBB[Band, Bin]


    def pullAllArms(self):
        pass

    def sortArms(self):
        self.estBestArms = np.flip(np.argsort(self.lam_est))
        self.estBestValues = self.lam_est[self.estBestArms]



    def printPolicyInfo(self):
        self.sortArms()
        np.set_printoptions(precision=2)
        print("*" * 20)
        print(f"### POLICY INFORMATION")
        print(f"Policy: {type(self).__name__}")
        for Band in range(self.nbBands):
            print(f"#### Band ({Band})")
            posterior = self.posteriorBand[Band]
            print(f"Initial likelihood params: {type(posterior).__name__}(loc = {posterior._min}, scale = {posterior._scale})")
            print(f"Final likelihood params: {type(posterior).__name__}(loc = {posterior.min:.2f}, scale = {posterior.scale}) ")
            print(f"Estimated bin reward means: {self.lam_estBB[Band,:]}")
            print("*"*20)

        estBestArm0 = self.getBB(self.estBestArms[0])
        estBestvalue0 = self.estBestValues[0]
        estBestArm1 = self.getBB(self.estBestArms[1])
        estBestvalue1 = self.estBestValues[1]
        gap = estBestvalue0-estBestvalue1
        print(f"### Estimation Gap")
        print(f"Estimated 1st best Band/Bin = {estBestArm0} w/ value = {estBestvalue0}")
        print(f"Estimated 2nd best Band/Bin = {estBestArm1} w/ value = {estBestvalue1}")
        print(f"Gap = {gap}")
        print()
        self.printStageTimeline()


    def printStageTimeline(self):
        print()




        # Get best arm and value
        # Get second best arm and value
        # get gap
        # Get when the policy switched to Band, and Bin stages



