"""
Based class for al Hierarchical MAB (HMAB) policies
"""
from  SMPyBandits.Distribution import *
from .BasePolicy import BasePolicy
import numpy as np
from scipy import stats

class HMABPolicy(BasePolicy):
    def __init__(self, nbBands, nbBins, DistributionSetup = None, verbose = True):
        """

        :param nbBands:
        :param nbBins:
        :param posterior: <SMPyBandits Posterior>
        :param priorParams: <dict> of initial parameters for our posterior
        """
        self.nbBands = nbBands
        self.nbBins = nbBins
        self.nbArms = nbBins*nbBands  # Arms is the linear version of Band/Bin indexing
        self.DistributionSetup = DistributionSetup
        self.Bands = [None]*nbBands
        self.Bins = [None]*nbBins*nbBands
        self.isHMAB = True
        self.verbose = verbose
        self.nbDraws = 1
        self.CI = False
        self.STOP = False


        ### NOT TO BE EDITED - used by evaluator functions
        # ________________________________________________
        self.t = -1  #: Internal time
        self.pulls = np.zeros(self.nbArms, dtype= int)      #Count of pulls for each arm
        self.rewards = np.zeros(self.nbArms)    # Sum of rewards accumulated for each arm
        # ________________________________________________




    def getReward(self,arm, reward):
        # DON'T MODIFY INPUT ARGUMENTS FOR THIS AS EVALUATOR USES IT
        if isinstance(arm, (tuple, list, np.ndarray)):
            Band = arm[0]
            Bin = arm[1]
            arm = self.getArm(Band, Bin)
        else:
            (Band,Bin) = self.getBB(arm)
        if self.nbDraws > 1: # If there are more than 1 draw per bin selection
            self.t += reward.__len__()
            self.pulls[arm] += reward.__len__()
            self.rewards[arm] += np.sum(reward)
            self.Bands[Band].getReward(Bin,reward)
        else:
            self.t += 1
            self.rewards[arm] +=reward
            self.pulls[arm] += 1
            self.Bands[Band].getReward(Bin,reward)
        # self.updateDists()
        return self.rewards[arm]

    def seeArmReward(self,Band, Bin):
        return self.rewards[self.getArm(Band,Bin)]

    def getArm(self,Band, Bin):
        return np.ravel_multi_index((Band, Bin), (self.nbBands,self.nbBins))

    def getArms(self,Band, Bins):
        arms = []
        for Bin in Bins:
            arms.append(self.getArm(Band,Bin))
        return arms
    def getAllArms(self, Band):
        arms = []
        for bin in range(self.nbBins):
            arms.append(self.getArm(Band, bin))
        return arms

    def getBB(self,arm):
        return np.unravel_index(arm, (self.nbBands,self.nbBins))

    def getSampled(self, Band, min = 1):
        # Returns the arm indices (not Band,Bin) of all bins that have been sampled >= min times in the passed Band
        BandArms = self.getAllArms(Band)
        ValidBins = np.where(self.pulls[BandArms] >= min )[0]
        ValidArms = self.getArms(Band,ValidBins)
        return ValidArms

    def __str__(self):
        """ -> str"""
        return self.__class__.__name__

    def startGame(self):
        """ Start the game (fill pulls and rewards with 0)."""
        self.t = 0
        self.pulls.fill(0)
        self.rewards.fill(0)




    def choice(self): #Need this to remain choice for evaluator()
        """ Not defined."""
        raise NotImplementedError(
            "This method choice() has to be implemented in the child class inheriting from BasePolicy.")

    def getStats(self):
        self.mus = np.zeros(self.Bins.__len__())
        self.sigmas = np.zeros(self.Bins.__len__())
        for bin in range(self.mus.shape[0]):
            self.mus[bin] = self.Bins[bin].mu
            self.sigmas[bin] = self.Bins[bin].sigma
        self.bestArms = np.argsort(-self.mus)
        self.BBsorted= np.array([self.getBB(x) for x in self.bestArms]).transpose()
        self.mus_sorted = self.mus[self.bestArms]
        self.sigma_sorted = self.sigmas[self.bestArms]

        test = 1

    def printTopKBins(self,k, samples = False):
        Mu = []
        for i in range(k):
            if samples:
                print(f"(Band, Bin) = {self.Bins[self.bestArms[i]].__str__()} - Samples = {self.Bins[self.bestArms[i]].pulls}")
            else:
                print(f"(Band, Bin) = {self.Bins[self.bestArms[i]].__str__()}")
            Mu.append(self.Bins[self.bestArms[i]].mu)
        return Mu

    def printBins(self, ArmVec, samples = False):
        for Arm in ArmVec:
            if samples:
                print(f"(Band, Bin) = {self.Bins[Arm].__str__()} - Samples = {self.Bins[Arm].pulls}")
            else:
                print(f"(Band, Bin) = {self.Bins[Arm].__str__()}")

    def printBandEsts(self, percentile = False):
        if percentile:
            p_vec = []
            for Band in self.Bands:
                print(f"{Band.__str__()} - {self.percentile} Percentile = {round(Band.percentile,2)}")
                p_vec.append(round(Band.percentile,2))
            return p_vec
        else:
            for Band in self.Bands:
                print(Band.__str__())




    class Band:
        def __init__(self, Policy, index, nbBins, distribution, initParams, CI):
            self.Policy = Policy
            self.nbBins = nbBins
            self.index = index
            self.Dist = distribution(initParams)
            self.Mu = initParams[0]
            self.Sigma = initParams[1]
            self.Bins = [None]*nbBins
            self.pulls = 0
            self.sumRewards = 0
            self.BinSampled = []
            self.estBinMeans = [None]*nbBins
            self.percentile = None
            self.CI = CI


        def updateDistribution(self):
            # binMeans = np.array([self.estBinMeans[x] for x in np.where(self.BinSampled)[0].tolist()])
            binMeans = [self.estBinMeans[x] for x in self.BinSampled]
            self.Mu = np.mean(binMeans)
            self.Sigma = np.std(binMeans)
            self.Dist.reset(self.Mu, self.Sigma, 0,0,0,0)



        def getReward(self, bin, reward):
            if isinstance(reward,float): reward = np.array([reward])
            self.pulls += reward.shape[0]
            self.sumRewards += np.sum(reward)
            self.Bins[bin].addReward(reward)
            self.updateDistribution()
            self.estimatePercentile()

        def estimatePercentile(self):
            self.percentile = self.Dist.getPercentile(self.Policy.percentile)## Based on the current estimated distribution, get the Policy.quantile value
            return self.percentile

        def computeLawless(self, confidence):
            # estimates CI on the pth percentile
            Zp = stat.norm.ppf(confidence)
            #self.mu - stats.t
        def __str__(self):
            return f"Band [{self.index}] ED ~  {self.Dist.__str__()}"





    class Bin:
        def __init__(self, Band, bindex, Distribution, initParams, CI):
            self.Band = Band
            self.bindex = bindex
            self.Distribution = Distribution(initParams)
            self.pulls = 0
            self.sumReward = 0
            self.sumRewardSq =0
            self.rewardHistory = []
            self.mu = initParams[0]
            self.sigma = initParams[1]
            self.sampled = False
            self.CI = CI

        def addReward(self, reward):
            if isinstance(reward,float):reward = np.array([reward])
            self.pulls += reward.shape[0]
            self.sumReward += np.sum(reward)
            self.sumRewardSq += np.sum(np.square(reward)) # rewards squared, needed for variance estimation
            self.rewardHistory.extend(reward.tolist())

            if self.CI is not False: # If there is a value given for CI, compute the confidence interval
                mean, std = confidence_interval(self.rewardHistory, self.Band.Policy.confidence)
                self.mu, self.mu_l, self.mu_u = mean
                self.sigma, self.sigma_l, self.sigma_u = std
                self.Distribution.reset(self.mu, self.sigma, self.mu_l, self.mu_u, self.sigma_l, self.sigma_u)
            else:
                self.mu = self.sumReward/self.pulls
                self.sigma = np.sqrt((self.sumRewardSq - (self.sumReward**2)/self.pulls)/(self.pulls - 1))
                self.Distribution.reset(self.mu, self.sigma)
            if self.sampled is False:
                self.sampled = True
                self.Band.BinSampled.append(self.bindex)
            self.Band.estBinMeans[self.bindex] = self.mu


        def __str__(self):
            return f"({self.Band.index, self.bindex}) ~ {self.Distribution.__str__()}"

        def plot(self, ax, label = None):
            self.Distribution.plot(ax)
            return ax

def confidence_interval(data, confidence = .95):
    # Computes confidence intervals on the mean and standard deviation, in addition to the sample mean and sample standard deviation
    if data.__len__() <= 1:
        m = data[0]
        h = np.infty
        (s, h2l, h2u) = (np.nan, np.nan, np.nan)
    else:
        significance = 1-confidence
        a = 1.0 * np.array(data)
        n = len(a)
        m, se = np.mean(a), stats.sem(a)
        s2 = np.var(a, ddof = 1)
        df = n-1
        h = se * stats.t.ppf((1 + confidence) / 2., df) #half the confidence interval for the mean
        h2u = np.sqrt(df*s2/stats.chi2.ppf(significance/2,df))
        h2l = np.sqrt(df*s2/stats.chi2.ppf(confidence/2,df))
        s = np.sqrt(s2)


    return (m, m - h, m + h), (s, h2l, h2u)



    # def addPulls(self, *args, **kwargs):
    #     if isinstance(args, (tuple, list, np.ndarray)):
    #         Band = args[0]
    #         Bin = args[1]
    #         arm = self.getArm(Band, Bin)
    #     else:
    #         arm = args
    #     if "pulls" in kwargs.keys():
    #         pulls = kwargs["pulls"]
    #     else:
    #         pulls = 1
    #
    #     self.pulls[arm] += pulls
    #     return self.pulls[arm]
    #
    # def getPulls(self, *args):
    #     if isinstance(args, (tuple, list, np.ndarray)):
    #         Band = args[0]
    #         Bin = args[1]
    #         arm = self.getArm(Band, Bin)
    #     else:
    #         arm = args
    #     return self.pulls[arm]

    # def addReward(self, *args, **kwargs):
    #     if isinstance(args, (tuple, list, np.ndarray)):
    #         Band = args[0]
    #         Bin = args[1]
    #         arm = self.getArm(Band, Bin)
    #     else:
    #         arm = args
    #     if "reward" in kwargs.keys():
    #         reward = kwargs["reward"]
    #     else:
    #         Exception('"reward" Not passed as argument!!!')
    #     self.pulls[arm] +=
    #     self.rewards[arm] += reward


